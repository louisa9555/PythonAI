{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba15d722",
   "metadata": {},
   "source": [
    "# EP 2: Machine Learning 101\n",
    "\n",
    "üìö ‡∏™‡∏ô‡πÉ‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ô/‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤(Fastwork): <a href=\"https://fastwork.co/user/krittachailouis/tutoring-12693427\">https://fastwork.co/user/krittachailouis/tutoring-12693427</a>\n",
    "\n",
    "<img width=\"32\" alt=\"YouTube icon\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/32px-YouTube_full-color_icon_%282017%29.svg.png?20240107144800\"> Youtube:\n",
    " <a href=\"https://commons.wikimedia.org/wiki/File:YouTube_full-color_icon_(2017).svg\">\n",
    "  Louis MakerLab\n",
    " </a>\n",
    "\n",
    "<img width=\"32\" alt=\"GitHub icon\" src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"> GitHub: \n",
    "<a href=\"https://github.com/louisa9555\">\n",
    "  Louis MakerLab\n",
    "</a>\n",
    "\n",
    "\n",
    "## Linear Models, Tree-based Models ‡πÅ‡∏•‡∏∞ Kernel Methods\n",
    "### ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ô‡∏µ‡πâ:\n",
    "1. **Linear Models (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô)**\n",
    "   - Linear Regression (‡∏Å‡∏≤‡∏£‡∏ñ‡∏î‡∏ñ‡∏≠‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô)\n",
    "   - Logistic Regression (‡∏Å‡∏≤‡∏£‡∏ñ‡∏î‡∏ñ‡∏≠‡∏¢‡πÇ‡∏•‡∏à‡∏¥‡∏™‡∏ï‡∏¥‡∏Å)\n",
    "2. **Tree-based Models (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ)**\n",
    "   - Decision Trees (‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à)\n",
    "   - Random Forests (‡∏õ‡πà‡∏≤‡∏™‡∏∏‡πà‡∏°)\n",
    "   - XGBoost\n",
    "3. **Kernel Methods (‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏Ñ‡∏≠‡∏£‡πå‡πÄ‡∏ô‡∏•)**\n",
    "   - Support Vector Machine (SVM)\n",
    "   - Gaussian Processes\n",
    "---\n",
    "\n",
    "### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:\n",
    "- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó\n",
    "- ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\n",
    "- ‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î Python ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "- ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d23a1",
   "metadata": {},
   "source": [
    "## ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á\n",
    "- ‡∏™‡∏£‡πâ‡∏≤‡∏á conda env\n",
    "- `conda install -c conda-forge llvm-openmp`\n",
    "- `brew install libomp`\n",
    "- `pip install numpy pandas matplotlib seaborn scikit-learn xgboost statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import joblib\n",
    "from sklearn.datasets import make_classification, make_regression, load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import metrics\n",
    "# Linear Models\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# Tree-based Models\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# Kernel Methods\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier, GaussianProcessRegressor\n",
    "\n",
    "# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ matplotlib ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
    "import matplotlib.font_manager as fm\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß!\")\n",
    "\n",
    "# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡πÑ‡∏ó‡∏¢‡πÉ‡∏ô matplotlib\n",
    "def setup_matplotlib_thai_font():\n",
    "    \"\"\"Find and return a Thai font if available\"\"\"\n",
    "    # Candidate font names that support Thai\n",
    "    thai_fonts = [\"TH Sarabun New\", \"Noto Sans Thai\", \"Noto Serif Thai\", \"Tahoma\", \"Angsana New\"]\n",
    "\n",
    "    # Get list of installed fonts\n",
    "    font_list = fm.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "    for font_path in font_list:\n",
    "        prop = fm.FontProperties(fname=font_path)\n",
    "        try:\n",
    "            name = prop.get_name()\n",
    "            if any(tf in name for tf in thai_fonts):\n",
    "                print(f\"‚úî Using Thai font: {name}\")\n",
    "                return prop\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(\"‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡πÑ‡∏ó‡∏¢, ‡πÉ‡∏ä‡πâ‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô.\")\n",
    "    return None\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ü‡∏≠‡∏ô‡∏ï‡πå matplotlib\n",
    "matplotlib_thai_font = setup_matplotlib_thai_font()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cecd0d",
   "metadata": {},
   "source": [
    "## ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: Linear Models (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô)\n",
    "<div style=\"display:flex; justify-content:center;\">\n",
    "  <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231129130431/11111111.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "<div style=\"display:flex; justify-content:center; margin-top:10px;\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/0*nKdtnOmCfBCgC7on.jpg\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "### 1.1 Linear Regression (‡∏Å‡∏≤‡∏£‡∏ñ‡∏î‡∏ñ‡∏≠‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô)\n",
    "\n",
    "**‡∏ó‡∏§‡∏©‡∏é‡∏µ:**\n",
    "- Linear Regression ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (regression)\n",
    "- ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏≠‡∏¥‡∏™‡∏£‡∏∞ (features) ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ï‡∏≤‡∏° (target)\n",
    "- ‡∏™‡∏°‡∏Å‡∏≤‡∏£: $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon$\n",
    "- **‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å** ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å Linear Regression ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏´‡∏≤ **slope ‡πÅ‡∏•‡∏∞ Intercept** ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "\n",
    "**‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á:**\n",
    "- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß (‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ô‡πâ‡∏≠‡∏¢)\n",
    "- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢\n",
    "- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏°‡∏≤‡∏Å\n",
    "\n",
    "**‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô:**\n",
    "- ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô(linear)‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
    "- ‡πÑ‡∏ß‡∏ï‡πà‡∏≠ outliers\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ê‡∏≤‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏£ (linearity, independence, etc.)\n",
    "\n",
    "Reference: https://www.freecodecamp.org/news/machine-learning-mean-squared-error-regression-line-c7dde9a26b93/ , \n",
    "https://www.geeksforgeeks.org/machine-learning/ml-linear-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Linear Regression\n",
    "X_reg, y_reg = make_regression(n_samples=100, n_features=1, noise=20)\n",
    "df = pd.DataFrame()\n",
    "df['X'] = X_reg.flatten()\n",
    "df['y'] = y_reg\n",
    "print(\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Linear Regression:\")\n",
    "display(df.head(10))\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å-‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    df[['X']], df['y'], test_size=0.2\n",
    ")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Linear Regression\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train_reg, y_train_reg)\n",
    "# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•\n",
    "y_pred_reg = linear_reg.predict(X_test_reg)\n",
    "print(y_pred_reg)\n",
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Mean Squared Error\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Slope: {linear_reg.coef_[0]:.2f}\")\n",
    "print(f\"Intercept: {linear_reg.intercept_:.2f}\")\n",
    "plt.show()\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train_reg, y_train_reg, alpha=0.7, label='‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train')\n",
    "plt.scatter(X_test_reg, y_test_reg, alpha=0.7, color='red', label='‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• test')\n",
    "X_plot = np.linspace(df['X'].min(), df['X'].max(), 100).reshape(-1, 1)\n",
    "df_X_plot = pd.DataFrame(X_plot, columns=['X'])\n",
    "y_plot = linear_reg.predict(df_X_plot)\n",
    "plt.plot(X_plot, y_plot, 'g-', linewidth=2, label='‡πÄ‡∏™‡πâ‡∏ô regression')\n",
    "plt.xlabel('X', fontproperties=matplotlib_thai_font)\n",
    "plt.ylabel('y', fontproperties=matplotlib_thai_font)\n",
    "plt.title('Linear Regression', fontproperties=matplotlib_thai_font)\n",
    "plt.legend(prop=matplotlib_thai_font)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test_reg, y_pred_reg, alpha=0.7)\n",
    "plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--', linewidth=2)\n",
    "plt.xlabel('‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á', fontproperties=matplotlib_thai_font)\n",
    "plt.ylabel('‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢', fontproperties=matplotlib_thai_font)\n",
    "plt.title('‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á vs ‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢', fontproperties=matplotlib_thai_font)\n",
    "for i in range(len(y_test_reg)):\n",
    "    plt.plot([y_test_reg.iloc[i], y_test_reg.iloc[i]], [y_test_reg.iloc[i], y_pred_reg[i]], 'k-', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£\n",
    "# 3 features: X1, X2, X3\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=150, n_features=3, noise=25\n",
    ")\n",
    "df = pd.DataFrame(X_reg, columns=['X1', 'X2', 'X3'])\n",
    "df['y'] = y_reg\n",
    "print(\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Multiple Linear Regression:\")\n",
    "display(df.head())\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô train-test\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    df[['X1', 'X2', 'X3']], df['y'], test_size=0.2\n",
    ")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• Linear Regression\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•\n",
    "y_pred_reg = linear_reg.predict(X_test_reg)\n",
    "\n",
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Mean Squared Error\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Intercept: {linear_reg.intercept_:.2f}\")\n",
    "print(f\"Coefficients: {linear_reg.coef_}\")\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á vs ‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test_reg, y_pred_reg, alpha=0.7, color='royalblue')\n",
    "plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--', linewidth=2)\n",
    "plt.xlabel('‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á', fontproperties=matplotlib_thai_font)\n",
    "plt.ylabel('‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢', fontproperties=matplotlib_thai_font)\n",
    "plt.title('Multiple Linear Regression\\n‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á vs ‡∏Ñ‡πà‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢', fontproperties=matplotlib_thai_font)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec400e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(linear_reg, \"models/linear_reg_model.pkl\")\n",
    "print(\"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î model\n",
    "loaded_model = joblib.load(\"models/linear_reg_model.pkl\")\n",
    "# print(type(loaded_model))\n",
    "# Predict\n",
    "y_pred = loaded_model.predict(X_test_reg)\n",
    "\n",
    "# Check model parameters\n",
    "print(\"Model ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß!\")\n",
    "print(\"Coefficients:\", loaded_model.coef_)\n",
    "print(\"Intercept:\", loaded_model.intercept_)\n",
    "print(\"R¬≤ Score:\", loaded_model.score(X_test_reg, y_test_reg))\n",
    "display(pd.DataFrame({'Actual': y_test_reg, 'Predicted': y_pred}).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79569de8",
   "metadata": {},
   "source": [
    "### 1.2 Logistic Regression (‡∏Å‡∏≤‡∏£‡∏ñ‡∏î‡∏ñ‡∏≠‡∏¢‡πÇ‡∏•‡∏à‡∏¥‡∏™‡∏ï‡∏¥‡∏Å)\n",
    "Reference: https://www.geeksforgeeks.org/machine-learning/understanding-logistic-regression/\n",
    "<div style=\"display:flex; justify-content:center;\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cb/Exam_pass_logistic_curve.svg\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "**‡∏ó‡∏§‡∏©‡∏é‡∏µ:**\n",
    "- Logistic Regression ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó (classification)\n",
    "- ‡πÉ‡∏ä‡πâ sigmoid function ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0-1 (S-shape curve)\n",
    "- ‡∏™‡∏°‡∏Å‡∏≤‡∏£: $p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n)}}$\n",
    "- **‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å** ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å Logistic Regression ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏´‡∏≤ **‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å (Weights ‡∏´‡∏£‡∏∑‡∏≠ $\\beta$-coefficients)** ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "\n",
    "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ \\(p\\) ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏£‡∏¥‡∏á (Actual Outcomes) ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "\n",
    "**‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á:**\n",
    "- ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô\n",
    "- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ê‡∏≤‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "- ‡∏ó‡∏ô‡∏ï‡πà‡∏≠ outliers ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ Linear Regression\n",
    "\n",
    "**‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô:**\n",
    "- ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô\n",
    "- ‡∏≠‡∏≤‡∏à‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô\n",
    "\n",
    "Data: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database?resource=download\n",
    "\n",
    "Documents: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86f3026",
   "metadata": {},
   "source": [
    "| Solver              | Algorithm Type                | Best For            | Penalty Support     | Multi-class Support | Pros                                      | Cons                                     |\n",
    "| ------------------- | ----------------------------- | ------------------- | ------------------- | ------------------- | ----------------------------------------- | ---------------------------------------- |\n",
    "| **lbfgs**           | Quasi-Newton (L-BFGS)         | Medium datasets     | L2, none            | OvR, Multinomial    | Robust, default choice, accurate          | Higher memory use on very large datasets |\n",
    "| **liblinear**       | Coordinate descent (like SVM) | Small/medium data   | L1, L2              | OvR only            | Works with sparse data, supports L1       | No multinomial, slower on large data     |\n",
    "| **newton-cg**       | Newton‚Äôs method (conjugate)   | Medium/large data   | L2, none            | OvR, Multinomial    | Accurate for multi-class                  | Slower than lbfgs                        |\n",
    "| **newton-cholesky** | Newton‚Äôs method (Cholesky)    | Medium/large data   | L2, none            | OvR, Multinomial    | Stable, can be faster than newton-cg      | Memory-intensive                         |\n",
    "| **sag**             | Stochastic Average Gradient   | Large datasets      | L2, none            | OvR, Multinomial    | Very fast on large datasets               | Needs scaling, unstable on small data    |\n",
    "| **saga**            | Stochastic Average Gradient + | Large/high-dim data | L1, L2, Elastic Net | OvR, Multinomial    | Most flexible (supports Elastic Net & L1) | Slower than lbfgs on small datasets      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290b826",
   "metadata": {},
   "source": [
    "#### ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc6f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/diabetes.csv')\n",
    "display(df.shape)\n",
    "\n",
    "# ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå\n",
    "empty = df.isnull().sum().any()\n",
    "if empty:\n",
    "    print(\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå:\", df.isnull().sum())\n",
    "else:\n",
    "    print(\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 20))\n",
    "features = ['Pregnancies','Glucose','BloodPressure','SkinThickness',\n",
    "            'Insulin','BMI','DiabetesPedigreeFunction','Age']\n",
    "for i, col in enumerate(features, 1):\n",
    "    plt.subplot(4, 2, i)\n",
    "    sns.regplot(x=col, y='Outcome', data=df, logistic=True, ci=None, scatter_kws={'alpha':0.3})\n",
    "    plt.title(f'Logistic Relationship: {col} vs Outcome')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3885b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏ô‡πÉ‡∏à max_iter, penalty ['l1', 'l2', 'elasticnet'], \n",
    "# solver ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'], n_job (-1 ‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å processor)\n",
    "\n",
    "# --- Prepare data ---\n",
    "X = df.iloc[:, 0:8]\n",
    "y = df.iloc[:, 8]\n",
    "X_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3)\n",
    "print(\"X train shape:\", X_train.shape)\n",
    "print(\"X test shape:\", X_test.shape)\n",
    "print(\"y train shape:\", y_train.shape)\n",
    "print(\"y test shape:\", y_test.shape)\n",
    "\n",
    "# --- Compare penalties ---\n",
    "models = {\n",
    "    'L1': LogisticRegression(max_iter=500, penalty='l1', solver='liblinear'),\n",
    "    'L2': LogisticRegression(max_iter=500, penalty='l2', solver='saga'),\n",
    "    'ElasticNet': LogisticRegression(max_iter=500, penalty='elasticnet', solver='saga', l1_ratio=0.8)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"{name} model accuracy: {model.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e362a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Glucose','BMI','Age','Pregnancies']\n",
    "# --- Prepare data ---\n",
    "X = df[features]\n",
    "y = df['Outcome']\n",
    "X_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "print(\"X train shape:\", X_train.shape)\n",
    "print(\"X test shape:\", X_test.shape)\n",
    "print(\"y train shape:\", y_train.shape)\n",
    "print(\"y test shape:\", y_test.shape)\n",
    "\n",
    "# --- Compare penalties ---\n",
    "models = {\n",
    "    'L1': LogisticRegression(max_iter=500, penalty='l1', solver='liblinear'),\n",
    "    'L2': LogisticRegression(max_iter=500, penalty='l2', solver='saga'),\n",
    "    'ElasticNet': LogisticRegression(max_iter=500, penalty='elasticnet', solver='saga', l1_ratio=0.8)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"{name} model accuracy: {model.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e76350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏î‡∏π‡∏Ñ‡πà‡∏≤ coefficients\n",
    "for model in models.values():\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Coefficient': model.coef_[0]\n",
    "    }).sort_values('Coefficient', ascending=False)\n",
    "    print(coef_df)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "for name, model in models.items():\n",
    "    print(f'=== {name} model ===')\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Accuracy and reports\n",
    "    acc = model.score(X_test, y_test)\n",
    "    print(f'Accuracy: {acc:.3f}')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # print('Confusion Matrix:', cm)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=getattr(model, 'classes_', None))\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix for {name} Model')\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff24230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(models['L1'], \"models/logistic_reg_model.pkl\")\n",
    "print(\"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = joblib.load(\"models/logistic_reg_model.pkl\")\n",
    "print(\"Model ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß!\")\n",
    "# Predict\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "print(\"Coefficients:\", loaded_model.coef_)\n",
    "print(\"Intercept:\", loaded_model.intercept_)\n",
    "print(\"R¬≤ Score:\", loaded_model.score(X_test, y_test))\n",
    "display(pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}).head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
